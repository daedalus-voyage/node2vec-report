\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[colorlinks, linkcolor=black, citecolor=black, urlcolor=black]{hyperref}
\usepackage{geometry}
\geometry{tmargin=3cm, bmargin=2.2cm, lmargin=2.2cm, rmargin=2cm}
\usepackage{todonotes} %Used for the figure placeholders
\usepackage{ifthen}
\usepackage{parskip}
\usepackage{color,soul}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[font=footnotesize]{caption}
\setlength{\tabcolsep}{8pt}
\renewcommand{\arraystretch}{1.5}

\usepackage{glossaries}
\makeglossaries
\input{glossary}

\begin{document}
\newboolean{anonymize}
% Uncomment to create an anonymized version of your report
%\setboolean{anonymize}{true}

\input{titlepage}

\tableofcontents
\newpage

\section{Introduction}

Graphs networked nature presents challenges in applying classic machine learning algorithms that depend on the default setting expecting tabular data. This default setting works well for tasks where a fixed set of parameters is mapped to a fixed number of outcomes. Graph data cannot readily make use of such a setting because of the varying degrees of their nodes and by extension their varying structural connections.

Textual data has a similar challenge where its unbounded nature lacks the structure of the default setting. Word2Vec is an algorithm coping with this challenge by assuming the fixed immediate context of a word to be reflective of its semantic meaning. Using multiple examples of such word contexts, a word can be represented by a fixed length feature vector, the word embedding.

This project report discusses the Node2Vec algorithm that applies the ideas of Word2Vec to graph data for a node classification task. Different parameters of the algorithm are inspected against a common dataset and their performance is evaluated. In our conclusion, Node2Vec is compared to similar algorithms using the obtained results.

\subsection{Node2Vec}

Node2Vec is an unsupervised transductive machine learning algorithm to learn feature representation for nodes in a network. To achieve this, first a dataset of node sequences is collected by a biased random walker. Viewing the node id’s encountered in random walks as the words in a sentence, Word2Vec is then used to produce feature embeddings encoding a node’s context. Armed with these embeddings, a classifier can be trained to predict node properties. Alternatively, edge properties might be predicted by converting node embeddings into edge embeddings. In essence, Node2Vec uses a random walker to sample a node's context in order to generate a dataset compatible with Word2Vec.

Other approaches to sample the context of a node are depth-first (DFS) and breadth-first (BFS) sampling. DFS explores large parts of a network, creating a macro-view on the neighborhood of a node. This infers similarity based on \textit{homophily} where similar nodes belong to similar communities. DFS context exploration tends to not repeat nodes leading to characterizations with high variance.

BFS restricts exploration to the immediate surroundings of a node and tends to sample duplicate nodes, isolating a microsopic view with low variance in its characterization. BFS will therefore define similarity as a \textit{structural equivalence} based on the network role of a node (bridge node, hubs, etc.).

Node2Vec improves on these strategies by providing a configuration mechanism to choose which levels of \textit{homophilic} and \textit{structural equivalence} are proper for the task at hand. This approach promises a more realistic definition of similarity which is never accurately defined as either one of the described DFS and BFS approaches.

\subsection{Random walk parameters}

Instead of DF or BF sampling approaches, Node2Vec uses a biased random walker configured using 4 parameters: walk length, walk number, P, and Q values. For each  node in the graph, the \textit{graph number} defines how many random walks are performed. Every random walk samples new nodes until a maximum number of nodes, specified by a \textit{walk length} parameter.

The $2^{nd}$ order random walker configures the reach of the random walks using parameters $p$ and $q$. For a random walker traversing edge $(t, v)$ and currently residing at node $v$, $\pi_{vx}$ defines a transition probability on edges $(v, x)$ leading from $v$. Unnormalized, the transition probability calculates as $\pi_{v x}=\alpha_{p q}(t, x) \cdot w_{v x}$, with the search bias $\alpha$ taking values as follows:

$$
\alpha_{p q}(t, x)=\left\{\begin{array}{ll}
\frac{1}{p} & \text { if } d_{t x}=0 \\
1 & \text { if } d_{t x}=1 \\
\frac{1}{q} & \text { if } d_{t x}=2
\end{array}\right.
$$

Walking from $t$ first to $v$ and then to $x$, $d_{tx}$ represents the shortest path between $t$ and $x$. Intuitively, this means that $p$ and $q$ encode both the speed with which the walker explores and how easily it leaves the neighborhood. Tweaking these parameters allows the algorithm to interpolate between DFS and BFS approaches.

Lower q-values produce a higher probability of leaving the neighborhood, while higher q-values prefer sticking to the same neighborhood. The q-value is therefore also called the \textit{in-out parameter}.

Lower p-values create higher probabilities to backtrack on a walk, while higher values reduce the probability of redundant nodes in a walk. The p-value is also called the \textit{return paramter}.

This report researches what values for all 4 parameters produce the best results on a classification task.

\section{Method and dataset}

\subsection{Node2Vec datasets}

\begin{wrapfigure}{r}{0.35\textwidth}
  \centering
  \vspace{-8mm}
      \includegraphics[width=\linewidth]{./attachments/ppi-confusion.png}
        \caption{Confusion matrix for PPI dataset}
        \label{fig:ppi:confusion-matrix}
    % \end{figure}
\end{wrapfigure}

The requested and approved datasets are the ones used in the original Node2Vec paper. Both the wikipedia and protein interaction datasets were considered, which are available as `.mat` files containing a compressed sparse column matrix.

After loading the data and starting analysis, the Node2Vec algorithm experienced difficulty converging, producing inefficient node embeddings. Figure (\ref{fig:ppi:confusion-matrix}) shows the confusion matrix for one such attempted Node2Vec run, revealing failure to detect positive examples of the largest class in a highly unbalanced dataset. The dominance of negative samples allowed our classification to still obtain a high accuracy of 94\% while not learning features of interest. Balancing the dataset before classification did not reduce confusion.

An incorrect import of the data or possibly mixups of certain identifiers is suspected, but even after careful inspection and given the lack of detailed documentation of the original datasets on the SNAP webpages, the Cora dataset was selected instead. Debugging the conversion of storage formats was considered out-of-scope for this report.

\subsection{Cora dataset}

The Cora dataset represents a citation network containing 5429 undirected edges, each representing a citation between two of its 2708 scientific publication nodes (papers). Each node is represented by a one-dimensional feature vector of size 1433 indicating the presence of one of 1433 unique dictionary words in each individual paper. Each node is additionally labeled with a single class, making the dataset suitable for single label node classification.

The largest connected component contains 2485 nodes and 5209 edges, not much smaller than the full network, indicating a well connected network with few of its works isolated. This makes sense, as papers are written within collaborative networks consisting of universities and research groups. Figure (\ref{fig:cora:degree-distribution}) shows the degree distribution after removing 1\% of the most connected nodes for easier review of the graph. Most papers have a degree of less than 5 and only the top 1\% are more popularly cited. The highest degree of any node is 169.

Betweenness in the Cora dataset is skewed with a maximum of 850663, a mean of 6596 and a mode of 0.

\begin{figure}[!bp]
  \centering
  \begin{minipage}[b]{0.40\textwidth}
    \includegraphics[width=\linewidth]{./attachments/degree-distribution.png}
    \caption{Degree distribution Cora dataset (99\%)}
    \label{fig:cora:degree-distribution}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.40\textwidth}
    \includegraphics[width=\linewidth]{./attachments/betweenness.png}
    \caption{Betweenness distribution Cora dataset (full dataset)}
    \label{fig:cora:betweenness-distribution}
  \end{minipage}
\end{figure}

\section{Results}

\subsection{Walk length}

\begin{table}[!hbtp]

  \centering

  \begin{minipage}[b]{0.48\textwidth}

    \begin{tabular}{c c c c}

      \hline
      Walk length & F1 (micro)  & F1 (macro) \\
      \hline\hline

      3.0	& 0.2830	& 0.1081 \\
      \hline
      6.0	& 0.5238	& 0.4347 \\
      \hline
      9.0	& 0.6577	& 0.5877 \\
      \hline
      12.0	& 0.7160	& 0.6779 \\
      \hline

    \end{tabular}

    \caption{Walk length performance}
    \label{tbl:cora:performance:walk_length}

  \end{minipage}
\hfill
  \begin{minipage}[b]{0.48\textwidth}

    \begin{tabular}{c c c c}

      \hline
      Walk number & F1 (micro)  & F1 (macro) \\
      \hline\hline

      20.0	&  0.5299	&  0.4688 \\
      \hline
      40.0	&  0.7079	&  0.6800 \\
      \hline
      60.0	&  0.7455	&  0.7260 \\
      \hline
      80.0	&  0.7416	&  0.7141 \\
      \hline
      100.0	&  0.7669	&  0.7466 \\
      \hline
      120.0	&  0.7626	&  0.7365 \\
      \hline

    \end{tabular}

    \caption{Walk number performance}
    \label{tbl:cora:performance:walk_number}

  \end{minipage}

\end{table}

@todo: explain general evolution and the reasoning behind it

\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/walk_length.png}
    % \includegraphics[width=\linewidth]{./attachments/session-1/training-speed_noisy.png}
      \caption{Walk length performance}
      \label{fig:walk-length}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/walk_number.png}
    % \includegraphics[width=\linewidth]{./attachments/session-1/training-speed_noisy.png}
      \caption{Walk number performance}
      \label{fig:walk-number}
  \end{minipage}
\end{figure}

\subsection{Walk number}

@todo: explain general evolution and the reasoning behind it

\subsection{Q/P values}

@todo : Discuss Q/P value results (explain dips, general evolution, what it means)

\begin{table}[!tbp]

  \centering

  \begin{minipage}[b]{0.48\textwidth}

    \begin{tabular}{c c c c}

      \hline
      Q & F1 (micro)  & F1 (macro) \\ [0.5ex]
      \hline\hline

      0.01	& 0.7495	& 0.7230 \\ \hline
      0.1	& 0.7419	& 0.6960 \\ \hline
      0.5	& 0.7620	& 0.7421 \\ \hline
      1.0	& 0.7547	& 0.7342 \\ \hline
      5.0	& 0.7273	& 0.7055 \\ \hline
      10.0	& 0.6800	& 0.6259 \\ \hline
      50.0	& 0.6695	& 0.6485 \\ \hline

    \end{tabular}

    \caption{Q value performance}
    \label{tbl:cora:performance:q_values}

  \end{minipage}
\hfill
  \begin{minipage}[b]{0.48\textwidth}

    \begin{tabular}{c c c c}

      \hline
      P & F1 (micro)  & F1 (macro) \\ [0.5ex]
      \hline\hline

      0.01	& 0.5707	& 0.5025 \\ \hline
      0.1	& 0.7161	& 0.6618 \\ \hline
      0.5	& 0.7386	& 0.7171 \\ \hline
      1.0	& 0.7559	& 0.7345 \\ \hline
      5.0	& 0.7528	& 0.7277 \\ \hline
      10.0	& 0.7546	& 0.7277 \\ \hline
      50.0	& 0.7538	& 0.7075 \\ \hline

    \end{tabular}

    \caption{P value performance}
    \label{tbl:cora:performance:p_values}

  \end{minipage}

\end{table}

% P VALUE ANALYSIS
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/p_values_short.png}
    \caption{P values (for q=1.0)}
    \label{fig:q-values}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/p_values_long.png}
    \caption{P values (for q=1.0)}
    \label{fig:p-values}
  \end{minipage}
\end{figure}

% Q VALUE ANALYSIS
\begin{figure}[!tbp]
  \centering
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/q_values_short.png}
    \caption{Q values (for p=1.0)}
    \label{fig:q-values}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.49\textwidth}
    \includegraphics[width=\linewidth]{./attachments/q_values_long.png}
    \caption{Q values (for p=1.0)}
    \label{fig:p-values}
  \end{minipage}
\end{figure}

\section{Conclusion}

@todo : What is needed here? Check slides?
- focus on gained knowledge

% \section{Glossary}

% \printglossaries

\end{document}
